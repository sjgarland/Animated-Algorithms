.RH "The bisection algorithm revisited"
.IN "bisection algorithm"
.pp
In Chapter 4 we developed a simple method for approximating a
zero of a function <em>f</em>.  That method, the bisection
algorithm, is sure and steady.  Given an interval
(<em>a</em>,\|<em>b</em>) such that <em>f</em>(<em>a</em>) and
<em>f</em>(<em>b</em>) have opposite signs, it progressively cuts that
interval in half, trapping a zero of <em>f</em> between two points
that get closer and closer together.
.pp
\*(PR shows the sequences of approximations generated by the
bisection algorithm to zeroes of $x sup 2 ~-~4$ and
<em>sin</em>(<em>x</em>) in the interval (1,\|10).  These results were
produced by inserting the statement
<em>writeln</em>(\^<em>x</em>,\|<em>f</em>(<em>x</em>)\^) at the end of the
loop in the procedure <em>bisect2</em> invoked by <em>zeroDemo2</em>.
.SP program zeroDemo2 "(approximations produced by bisection method)" io1
.pp
Despite its simplicity and reliability, the bisection algorithm
is not without its drawbacks.  First, we must supply it with an
interval on which <em>f</em> changes sign.  This in itself may take
some work, and we might prefer an algorithm that requires less
information.  Second, though the bisection algorithm converges
steadily to a zero of <em>f</em>, it does not converge particularly
quickly.  To approximate $sqrt 2$ to within $10 sup -6$, it must
evaluate $x sup 2 ~-~ 2$ and cut the interval (0,\|10) in half
23 times.  Hence we might prefer an algorithm that converges
more rapidly.
.pp
Several possibilities suggest themselves for speeding up the
bisection algorithm.  In general, we would like to trap a zero
of <em>f</em> in an interval that shrinks as rapidly as possible.
We could, for example, construct a
.q trisection
algorithm that cuts an interval on which <em>f</em> changes sign into
three pieces and retains the piece in which the change of sign
occurs.  However, since we must evaluate <em>f</em> twice on each
iteration of this algorithm, such a trisection algorithm
actually requires more work than the bisection algorithm (see
Exercise 1).
.pp
A more promising approach is to cut an interval on which <em>f</em>
changes sign into two unequal pieces and hope to trap a zero of
<em>f</em> in the smaller of the pieces.  This idea leads to the
following algorithm.
.RH "The method of false position"
.pp
In the 
.i "method of false position,"
.IN "method of false position"
we cut an interval (<em>a</em>,\|<em>b</em>) in two by constructing the 
.i secant,
.IN secant
or line, joining the point <<em>a</em>,\|<em>f</em>(<em>a</em>)> to the
point <<em>b</em>,\|<em>f</em>(<em>b</em>)> and letting $m sub 1$ be the
intersection of this line with the <em>x</em>-axis (see \*(FR).
The slope <em>s</em> of the secant is given by the formula
.BS
$s ~~=~~ { f(b) ~-~ f(a) } over { b ~-~ a } ~.$
.BE
Since the point ($m sub 1$,\|0) lies on the secant, the slope is
also given by the formula
.BS
$ s ~~=~~ f(b) over { b ~-~ m sub 1 } ~.$
.BE
Hence
.BS
$m sub 1 ~~=~~ b ~-~ f(b) over s ~~=~~ b ~-~ f(b) \
{ { b ~-~ a } over { f(b) ~-~ f(a) } } ~.$
.BE
As in the bisection method, we can evaluate <em>f</em>($m sub 1$)
to determine whether <em>f</em> changes sign on (<em>a</em>,\|$m sub
1$) or ($m sub 1$,\|<em>b</em>), and we can subdivide that
subinterval further by constructing another secant.
.pp
\*(FR shows that the method of false position can trap a zero of
<em>f</em> much more quickly than bisection.
.FS
.LS
                 <em>f</em>        .      .
|              .              .  |
|        .                 .     |      
|    .  .|              .        |
|  .   . |           .           |
| .|  .  |        .              |
|  | .   |     .                 |
|. |.    |  .                    |
|................................|
| .   .                          |
|. .                             |
\&.
<em>a  m     m                       b</em>
    2     1
.LE
.FE "Fast convergence using method of false position"
.NO "(artwork) connect the dots in \*(FL, do one more iteration"
Unfortunately, as shown by \*(FR, the method of false position
can also converge much more slowly than the bisection algorithm.
This happens when the zero of <em>f</em> is trapped in the larger of
the two pieces rather than in the smaller.
.FS
.LS
                                 .
|                             .  |
|                          .     |
|                       .       .|
|                    .           |
|                 .              |
|              .               . |
|           .                    |
|................................|
|     .  |           .           |
|  .     .                       |
\&.
<em>a        m   m   m               b</em>
          1   2   3
.LE
.FE "Slow convergence using method of false position"
.NO "(artwork) connect the dots in \*(FL, do another iteration"
Thus, unless we know something special about the function <em>f</em>,
the method of false position has little to recommend it over the
method of bisection.  Consequently, we leave its implementation
as an exercise (see Exercise 2).
.pp
Still, the method of false position provides some insight into a
new approach to approximating zeroes.  What that method does is
to approximate a function <em>f</em> on the interval
(<em>a</em>,\|<em>b</em>) by a straight line \(em the secant \(em and
to use a zero of this approximate function as an approximation
to a zero of <em>f</em>.  If the secant is a good approximation to
<em>f</em>, then a zero of the secant will be a good approximation
to a zero of <em>f</em>.  This insight leads us to a third method
for approximating zeroes of functions.
.RH "The secant method"
.IN "secant method"
.pp
In the <em>secant method</em>, we abandon the strategy of trapping
a zero of <em>f</em> in intervals $( a sub 1,^ b sub 1 )$, $( a sub
2,^ b sub 2 )$, ... that get progressively smaller.  Instead, we
attempt to produce a sequence of approximations $x sub 1$, $x
sub 2$, ...  that get progressively closer to a zero of <em>f</em>.
.pp
As in the method of false position, we use secants to produce
our approximations.  Given two approximations $x sub n-1$ and $x
sub n$ to a zero of <em>f</em>, we construct the secant connecting
$< x sub n-1 ,^ f( x sub n-1 ) >$ to $< x sub n ,^ f( x sub n )
>$, and we let the next approximation $x sub n+1$ be the
intersection of this secant with the <em>x</em>-axis.  Recalling
the computation we performed for the method of false position,
we see that
.BS
$x sub n+1 ~~=~~ x sub n ~-~ f( x sub n ) {\
{ x sub n ^-^ x sub n-1 } over { f( x sub n ) ^-^ f( x sub n-1 ) }
}$  .
.BE
.pp
As \*(FR shows, the intersection of this secant need not lie
between $x sub n-1$ and $x sub n$.  But it usually lies closer
to a zero of <em>f</em>, and constructing further secants usually
produces even better approximations.
.FS
.LS
                                 .
|                             .  |
|                          .     |
|                       .       .|
|                    .           |
|                 .              |
|              .               . |
|           .                    |
|................................|
|     .  |        .   .          |
|  .     .                       |
\&.                                |

x        x                  x    x
 2        3                  4    1
.LE
.FE "Approximating a zero of <em>f</em> using the secant method"
.pp
We say that a new approximation generated using the secant
method 
.q usually
lies closer to a zero of <em>f</em> because there are cases in
which the secant method behaves rather badly.  \*(FR, for
example, shows a case in which the approximations generated by
the secant method get further and further away from a zero of
<em>f</em>.  Here, even though the secants are good approximations
to <em>f</em>, they lead away from and not towards a zero of
<em>f</em>.
.FS
.LS
|          .
|      .      .
|    .           .
|   .                .
|                          .
|  .                              .
|..................................
|  
| .
|
.LE
.FE "Divergent approximations using secant method"
.NO "(artwork) draw secant approximations in \*(FR"
Worse yet, as illustrated by \*(FR, the secant method may break
down completely by generating a secant that is parallel to the
<em>x</em>-axis.  Here the secant is not a very good approximation
to <em>f</em> at all.
.FS
.LS
                                     
|                         .   .          
|                     .           .              
|                   .................
|                   |           .   |
|               .   |       .       |
|                   |   .           |
|...................................|
|               .   |               |
|     .     .       |               |
|       .           |               |
|   .               |               |
\&. 
<em>x                   x               x</em>
 1                   3               2
.LE
.FE "Horizontal secant generated using secant method"
.pp
Such anomalies are the price we pay to construct an algorithm
that, most of the time, converges more quickly than the
bisection algorithm.  In order to see how much quicker the
secant method is than bisection, and to see what precautions we
can take to guard against anomalous situations, we construct a
new function <em>zero</em> to approximate a zero of a given
function <em>f</em> by the secant method, and we package this
function in a library <em>secant</em>, displayed as \*(PR.
.SP library secant
.NO "use relative difference in secant method?"
.pp
The major problem in defining <em>zero</em> concerns when to
terminate the approximation process.  Since we are no longer
trapping a zero of <em>f</em> within smaller and smaller intervals,
we have no easy way to guarantee that an approximation is within
a specified tolerance of an actual zero of <em>f</em>.  But we can
stop the approximation process when two successive
approximations are very close together.  Intuitively, if our
approximations to a zero of <em>f</em> are getting closer and
closer together, then they must be getting closer and closer to
a zero of <em>f</em>.  Exercises in Section 10.5 provide some
mathematical justification for this stopping criterion.
.pp
As a precaution against infinite loops, we also stop the
approximation process if we reach a fixed limit <em>maxApprox</em>
on the number of iterations.  Since 20 iterations of the
bisection algorithm reduce an interval of size 1 to one of size
$10 sup -6$, and since we hope that the secant method requires
fewer iterations than bisection, we may as well give up if the
method does not converge in 20 iterations.  Finally, we also
guard against horizontal secants by taking an alternative
corrective action when they arise.
.pp
That the secant method is really faster than bisection is
illustrated by \*(PR, which was generated by <em>zeroDemo2</em> and
the version of <em>zero</em> contained in <em>secant</em>.  Whereas
the bisection algorithm required 23 iterations to approximate a
zero of either $x sup 2 ~-~ 4$ or $sin(x)$ to within $10 sup
-6$, the secant method requires only 7 or 8.
.SP program zeroDemo2 "(approximations generated by secant method)" io2
Two anomalies in \*(PR merit attention.  First, since the secant
method does not restrict its search for a zero to the interval
between the first two approximations, it actually produces a
different zero of <em>sin</em> than does the bisection method.  And
second, though the secant method appears to have found the zero
2 of <em>f</em>(<em>x</em>)\ = $x sup 2 ~-~ 4$ exactly, it reports a
curious value for <em>f</em>(2).  We examine why in Section 10.5.